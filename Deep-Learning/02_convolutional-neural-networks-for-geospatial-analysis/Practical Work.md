---
publishable: false
tags: []
date_updated: 2025-07-22
author: Generated by AI
---

# Exercises (satellite image analyist) â€“ Convolutional Neural Networks for Geospatial Analysis
> Here are 10 practical exercises to apply the concepts from the chapter "Convolutional Neural Networks for Geospatial Analysis" and prepare for the assessment. They are specifically designed for satellite image analyist.

---

> [!question]
> ### Exercise 1: Exploring Convolutional Filters on Sentinel-2 Imagery
> Given a pre-trained CNN model for land cover classification, your task is to visualize the filters (kernels) from the **first convolutional layer**.
> 1.  Load a pre-trained CNN model.
> 2.  Extract the weights of the first `Conv2D` layer.
> 3.  Display these filters as images.
> 4.  Analyze and describe what kind of features these initial filters are designed to detect in a Sentinel-2 image patch. For instance, do they detect ==edges==, ==corners==, ==specific color gradients==, or ==textures==?
>
> > [!TIP] Hint
> > The weights of a layer can usually be accessed with a function like `model.layers[0].get_weights()[0]`. Remember that the filter weights will have a shape like `(kernel_height, kernel_width, input_channels, output_channels)`. You'll need to iterate through the output channels to visualize each filter.

---

> [!question]
> ### Exercise 2: Geospatial-Specific Data Augmentation
> You are tasked with training a model to detect small rural buildings from high-resolution aerial imagery. Your dataset is limited, and the buildings appear under different lighting conditions and orientations.
>
> Propose and justify a `==data augmentation==` strategy. Your answer should:
> -   List at least **three** suitable augmentation techniques.
> -   Explain why each technique is relevant for this specific geospatial problem (e.g., rotational invariance).
> -   List at least **one** common augmentation technique you would *avoid* and explain why it might be detrimental (e.g., vertical flips in a context where building shadows must be consistent).
>
> > [!TIP] Hint
> > Think about the geometric and radiometric properties of aerial imagery. How do objects of interest and their environment vary in the real world? Consider techniques like `RandomRotation`, `RandomFlip`, and color adjustments like `ColorJitter`.

---

> [!question]
> ### Exercise 3: Patch-Based Land Cover Classification
> Your goal is to create a basic land cover map from a large Landsat 8 scene. You will use a simple CNN classifier.
>
> Describe the full workflow for a `==patch-based classification==` approach:
> 1.  How would you pre-process the large satellite scene and the corresponding ground truth label mask?
> 2.  How do you generate the training dataset of image patches? What is the role of `==stride==`?
> 3.  How do you use your trained CNN to classify the entire scene?
> 4.  What is a major drawback of this patch-based method, especially at the boundaries between classes?
>
> > [!TIP] Hint
> > The process involves "sliding" a window across the large image to extract small patches. The model classifies each patch individually, and the results are then reassembled to form the final map.

---

> [!question]
> ### Exercise 4: Transfer Learning with EuroSAT
> Compare the performance of two CNNs for classifying the 10 land use classes in the `==EuroSAT dataset==`.
>
> -   **Model A:** A simple CNN trained from scratch.
> -   **Model B:** A pre-trained `==ResNet50==` model (originally trained on ImageNet) where you apply `==fine-tuning==` to the top layers.
>
> You need to outline the steps for implementing Model B and predict which model will likely achieve higher accuracy and why. Your explanation should focus on the concept of `==feature extraction==`.
>
> > [!TIP] Hint
> > Fine-tuning involves "freezing" the early convolutional layers of the pre-trained model (which have learned general features like edges and textures) and only training the later, more specialized layers, plus a new classification head.

---

> [!question]
> ### Exercise 5: Calculating the Receptive Field
> Consider a simple CNN with the following architecture:
> -   `Conv1`: 3x3 kernel, stride 1
> -   `MaxPool1`: 2x2 pool size, stride 2
> -   `Conv2`: 3x3 kernel, stride 1
> -   `MaxPool2`: 2x2 pool size, stride 2
>
> Calculate the `==receptive field==` of a neuron in the final feature map (`Conv2` output). Explain what this size implies for detecting objects of different scales, such as cars (e.g., 5x5 pixels) versus shipping containers (e.g., 15x15 pixels) in a satellite image.
>
> > [!TIP] Hint
> > The receptive field is the size of the region in the input image that a particular CNN feature is looking at. You can calculate it layer by layer, starting from the end and working your way backward. The formula is `RF_out = RF_in + (kernel_size - 1) * stride`.

---

> [!question]
> ### Exercise 6: Semantic Segmentation for Burn Scar Mapping
> You need to map the area affected by a wildfire using Sentinel-2 imagery. This is a `==semantic segmentation==` task.
>
> Propose a CNN architecture suitable for this task. Your answer must:
> 1.  Name a specific, well-known architecture for semantic segmentation.
> 2.  Draw a simple diagram or describe its key components (e.g., encoder, decoder).
> 3.  Explain *why* this architecture is superior to a simple patch-based classification for delineating the precise boundaries of the `==burn scar==`.
>
> > [!TIP] Hint
> > Think about architectures that produce a dense, pixel-wise output map, not just a single label for an entire patch. The `U-Net` architecture is a very common choice in remote sensing.

---

> [!question]
> ### Exercise 7: Adapting a CNN for Multispectral Imagery
> Standard CNNs like VGG16 or ResNet are designed for 3-channel (RGB) images. Your task is to use a pre-trained VGG16 to classify crop types using 6-band Sentinel-2 data (e.g., Blue, Green, Red, NIR, Red Edge 1, Red Edge 2).
>
> How would you modify the network to handle `==multispectral data==` with 6 input channels?
> -   Describe the specific change needed in the `==first convolutional layer==`.
> -   Discuss the main trade-off of your proposed modification regarding the use of pre-trained weights.
>
> > [!TIP] Hint
> > The weights of a pre-trained model are a valuable starting point. How can you adapt the first layer to the new input shape while retaining the pre-trained knowledge from the other 99% of the network? One common strategy involves modifying the weights of the first layer.

---

> [!question]
> ### Exercise 8: Object Detection using a CNN and Sliding Window
> Your mission is to detect all cargo ships in a large satellite image of a port. You have a trained CNN that can classify a 64x64 pixel image patch as either 'ship' or 'no-ship'.
>
> Explain how you would use this classifier in a `==sliding window==` approach to find the locations of all ships.
> -   Describe the process, including the concepts of `==window size==` and `==stride==`.
> -   Identify two major challenges or limitations of this method, particularly in a real-world scenario with ships of varying sizes and high computational demands.
>
> > [!TIP] Hint
> > This is a "brute-force" detection method. The key is to systematically scan the entire image. Consider what happens when a window only partially covers a ship or when multiple windows detect the same ship.

---

> [!question]
> ### Exercise 9: Designing a Change Detection Network
> You need to detect new building constructions between two co-registered high-resolution satellite images taken one year apart.
>
> Outline the architecture of a `==Siamese Network==` for this `==change detection==` task.
> 1.  What would be the input to the network?
> 2.  How are the weights shared (or not shared) between the two processing streams?
> 3.  How are the features from the two images combined to produce the final change map?
>
> > [!TIP] Hint
> > A Siamese Network uses two identical sub-networks to process two different inputs. The goal is to learn a feature representation where similar inputs are mapped closer together. The final change map is often generated by comparing the feature vectors from the two dates.

---

> [!question]
> ### Exercise 10: Model Evaluation and Explainability (XAI)
> You have trained a U-Net model for forest segmentation. The model achieved 94% pixel accuracy. However, your manager is not satisfied and wants a deeper analysis.
>
> 1.  Suggest two evaluation metrics more suitable than pixel accuracy for a `==segmentation task==`, especially if the 'forest' class is much smaller than the 'non-forest' class. Explain your choices.
> 2.  You notice the model sometimes misclassifies a dense agricultural field as a forest. How could you use an `==Explainable AI (XAI)==` technique like `==Grad-CAM==` to understand *why* the model is making this error? Describe what you would expect to see in the Grad-CAM output for a misclassified patch.
>
> > [!TIP] Hint
> > For metrics, consider those that handle class imbalance well, like `Intersection over Union (IoU)` or the `Dice Coefficient`. For XAI, Grad-CAM produces a heatmap that highlights the regions of the input image that were most important for a specific prediction.