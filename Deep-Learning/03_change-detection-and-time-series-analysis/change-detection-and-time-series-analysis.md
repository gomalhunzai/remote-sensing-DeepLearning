---
publishable: false
tags: []
date_updated: 2025-07-22
author: Generated by AI
---

# Change Detection and Time-Series Analysis
> **Learning Objective:** Leverage recurrent and transformer-based architectures to analyze sequences of satellite images, enabling the monitoring of phenomena like deforestation, urban growth, and post-disaster recovery.

## Introduction
While single-image analysis provides a static snapshot of the Earth's surface, the true power of remote sensing is unlocked when we introduce the dimension of time. The Earth is a dynamic system, and phenomena such as deforestation, agricultural cycles, glacial melt, and urban expansion are processes, not events. To understand them, we must analyze sequences of images. This chapter moves beyond static analysis to explore deep learning architectures specifically designed for sequential data. We will delve into how Recurrent Neural Networks (RNNs) can process images one after another, maintaining a "memory" of past states to inform the present. We will then advance to Transformer models, which revolutionize sequence analysis by processing all time steps simultaneously, allowing them to capture complex, long-range dependencies between distant points in time. By the end of this chapter, you will understand the theoretical underpinnings and practical application of these models for creating dynamic, data-driven narratives of our changing planet.

---

## Topic 1: Sequential Analysis with Recurrent Neural Networks (RNNs)

At its core, change detection involves comparing two or more images of the same location taken at different times. The simplest approach, image differencing, often fails due to variations in illumination, sensor angle, or atmospheric conditions. Deep learning offers a more robust solution by learning to identify meaningful changes while ignoring irrelevant noise.

Recurrent Neural Networks (RNNs) are a class of neural networks designed to work with sequential data. Unlike a standard Convolutional Neural Network (CNN) that processes a single image in isolation, an RNN processes a sequence of inputs one step at a time, maintaining an internal "hidden state" or "memory" that captures information from previous steps.

Imagine analyzing a sequence of monthly images to track the construction of a new housing development.
-   At **Time 1 (January)**, the model sees a field. It encodes this information into its hidden state.
-   At **Time 2 (February)**, it processes the new image (which might show initial ground clearing) *and* receives the hidden state from January. Its new hidden state now contains information about both the "field" and the "clearing."
-   This continues for each month. The model's final output is informed by the entire sequence of events, allowing it to distinguish a new building from, say, a temporary flood.

**The Challenge of Long-Term Memory: LSTMs and GRUs**

Standard RNNs suffer from the "vanishing gradient problem," which makes it difficult for them to learn dependencies over long sequences. In our satellite imagery context, this means a basic RNN might "forget" what the land looked like a year ago, making it hard to detect slow, gradual changes like forest degradation.

To solve this, more sophisticated RNN variants were developed:
1.  **Long Short-Term Memory (LSTM):** LSTMs introduce a "cell state" and three "gates" (input, forget, and output). These gates are small neural networks that learn what information to add, what to discard, and what to output from the cell state. The **forget gate** is crucial; it can learn to discard irrelevant information (like cloud cover in one image) while retaining important long-term context (like the fact that an area was a dense forest six months ago).
2.  **Gated Recurrent Unit (GRU):** A simplified version of the LSTM, the GRU combines the input and forget gates into a single "update gate" and merges the cell state and hidden state. It is often computationally more efficient than an LSTM and delivers comparable performance on many tasks.

For satellite time-series analysis, you will almost always use LSTMs or GRUs instead of vanilla RNNs. They are the workhorses for modeling temporal evolution in land cover. A common architecture involves using a pre-trained CNN to extract spatial features from each image in the sequence, and then feeding this sequence of feature vectors into an LSTM or GRU to model the temporal dynamics.

> [!NOTE] Guided Application Exercise
> **Problem:** You have two multispectral satellite images of a coastal area, one from before a hurricane (T1) and one from after (T2). Your goal is to create a binary change map identifying areas of coastal erosion.
>
> **Resolution Process:**
> 1.  **Step 1: Data Preparation.** The two images (T1 and T2) must be co-registered, meaning they are perfectly aligned pixel-for-pixel. They should also be atmospherically corrected and normalized to ensure pixel values are comparable. We will treat these two images as a sequence of length two.
> 2.  **Step 2: Feature Extraction.** We use a pre-trained CNN (like a U-Net encoder) to process each image independently. For each pixel or patch, this CNN extracts a high-dimensional feature vector that represents its spatial context and spectral properties. For T1 we get Feature Vector `F1`, and for T2 we get `F2`.
> 3.  **Step 3: Sequential Modeling with a GRU.** We feed the sequence of feature vectors (`F1`, then `F2`) into a GRU layer.
    *   The GRU first processes `F1` and produces a hidden state `H1`.
    *   It then processes `F2` while also taking `H1` as input. This allows the model to directly compare the "after" state with its memory of the "before" state. The output of the GRU at this step, `H2`, is a feature representation that inherently encodes change.
> 4.  **Step 4: Classification.** The final hidden state `H2` is fed into a simple classification head (e.g., a fully connected layer with a sigmoid activation function) that outputs a probability of "change" for that pixel. By doing this for every pixel, we generate the final change map.
>
> **Solution:**
> ```python
> # Conceptual Python-like Pseudocode
>
> # 1. Load and pre-process data
> image_t1 = load_image("coastal_area_pre_hurricane.tif")
> image_t2 = load_image("coastal_area_post_hurricane.tif")
> # Assume images are co-registered and normalized
>
> # 2. Define the model components
> cnn_encoder = UNetEncoder(input_channels=13, output_features=256) # 13 Sentinel-2 bands
> gru_layer = GRU(input_features=256, hidden_features=128)
> classification_head = LinearLayer(input_features=128, output_classes=1)
> sigmoid = SigmoidActivation()
>
> # 3. Process the sequence
> # Extract spatial features from each image
> features_t1 = cnn_encoder(image_t1)
> features_t2 = cnn_encoder(image_t2)
>
> # Model temporal change with GRU (processing pixel-wise or patch-wise)
> # The hidden state is automatically passed from t1 to t2
> hidden_state_t1 = gru_layer(features_t1)
> change_features_t2 = gru_layer(features_t2, hidden_state_t1) # H2 encodes change
>
> # 4. Generate the final change map
> # Get a probability of change for each pixel
> change_logits = classification_head(change_features_t2)
> change_probability_map = sigmoid(change_logits)
>
> # The resulting map shows a high probability (e.g., > 0.5) for eroded areas.
> ```

---

## Topic 2: Global Context with Transformer-Based Architectures

While RNNs are powerful, their sequential nature presents two key limitations:
1.  **Computational Bottleneck:** They must process images one by one, which can be slow for very long time-series (e.g., 5 years of monthly data).
2.  **Long-Range Dependencies:** Even with LSTMs/GRUs, information can degrade over very long sequences. An RNN might struggle to connect an event in Year 1 with a consequence in Year 5.

The **Transformer** architecture, originally designed for natural language processing, overcomes these issues with a mechanism called **self-attention**.

Instead of processing data sequentially, a Transformer can look at the entire sequence of images at once. The self-attention mechanism allows each image in the sequence to weigh its importance relative to *every other image* in the sequence.

**How Self-Attention Works for Satellite Imagery**

1.  **Tokenization & Positional Encoding:** First, we process each satellite image in our time-series (e.g., 12 monthly images) through a CNN to get a feature vector for each. These are our "tokens." Since the Transformer architecture itself has no inherent sense of order, we add "positional encodings"â€”vectors that give the model information about the absolute or relative position of each image in the time-series (e.g., this is the 1st image, this is the 2nd, etc.).

2.  **Query, Key, and Value:** For each image's feature vector, the model generates three new vectors: a **Query (Q)**, a **Key (K)**, and a **Value (V)**.
    *   **Query:** Represents the current image's question: "What other images in this sequence are most relevant to me?"
    *   **Key:** Represents another image's label: "This is what I am."
    *   **Value:** Represents that other image's content: "This is the information I hold."

3.  **Calculating Attention Scores:** To find the relevance of Image 5 to Image 1, the model calculates the dot product between the Query of Image 1 (Q1) and the Key of Image 5 (K5). This is done for Q1 against all other Keys (K1, K2, ... K12). These scores are then normalized (using softmax) to create attention weights.

4.  **Applying Attention:** The final representation for Image 1 is a weighted sum of all the Value vectors (V1 to V12) in the sequence, where the weights are the attention scores calculated in the previous step.

**The result:** If a pixel shows a sudden drop in a vegetation index (like NDVI) in July (Image 7), the attention mechanism can directly link this event to an observed fire in June (Image 6) and a prolonged drought in April (Image 4), even if May's image was cloudy and irrelevant. It "attends" to the most informative moments in time, regardless of their distance, creating a much richer contextual understanding than a purely sequential RNN. This makes Transformers exceptionally well-suited for tasks like crop type classification (where early-season planting is critical) or long-term climate trend analysis.

> [!NOTE] Guided Application Exercise
> **Problem:** You have a time-series of 12 monthly NDVI images for an agricultural region. Your task is to classify the land cover type for each pixel (e.g., 'Deciduous Forest', 'Corn', 'Soybeans'). Different crops have unique growth profiles throughout the year.
>
> **Resolution Process:**
> 1.  **Step 1: Data Preparation.** Create a data "stack" where you have 12 NDVI images for the year, all co-registered. For each pixel, you will have a vector of 12 NDVI values, representing its phenological (growth) curve.
> 2.  **Step 2: Input Embedding and Positional Encoding.** Since we are using pixel vectors directly, our "embedding" is simply a linear layer that projects the 12-dimensional NDVI vector into a higher-dimensional space (e.g., 64 dimensions). We then add a positional encoding to this vector to inform the model of the month each NDVI value corresponds to.
> 3.  **Step 3: Transformer Encoder.** This sequence of 12 embedded vectors is fed into a Transformer Encoder layer. Inside this layer, the self-attention mechanism will calculate the relationships between every month for that pixel's growth curve. For a corn pixel, it might learn to pay high attention to the rapid green-up in June-July and the senescence in September. For a forest, it would learn a more stable profile.
> 4.  **Step 4: Classification.** The output of the Transformer Encoder is a sequence of context-aware vectors. We can average these vectors (or just use the vector corresponding to a special `[CLS]` token) and pass it to a final classification head (a linear layer with softmax) to predict the land cover class.
>
> **Solution:**
> ```python
> # Conceptual Python-like Pseudocode
>
> # 1. Load and prepare data
> # ndvi_timeseries is a stack of 12 images, shape (H, W, 12)
> # We process pixel by pixel, so input is a vector of shape (1, 12)
> pixel_ndvi_vector = ndvi_timeseries[y, x, :] # e.g., [0.2, 0.25, ..., 0.3]
>
> # 2. Define the model components
> # Projects 12-dim NDVI vector to 64-dim feature space
> input_embedding = LinearLayer(input_features=12, output_features=64)
> # Adds time information (Jan, Feb, etc.)
> positional_encoding = PositionalEncoding(dimensions=64, max_len=12)
> # The core of the model
> transformer_encoder = TransformerEncoderLayer(dimensions=64, num_heads=4)
> # Classifier for 5 crop types
> classification_head = LinearLayer(input_features=64, output_classes=5)
>
> # 3. Process the sequence
> # Embed the NDVI vector and add positional info
> embedded_vector = input_embedding(pixel_ndvi_vector)
> encoded_vector = positional_encoding(embedded_vector)
>
> # Pass through the transformer to get context-aware representation
> # The model learns which months are important for classification
> context_vector = transformer_encoder(encoded_vector)
>
> # We can average the output or use other pooling strategies
> final_representation = mean(context_vector)
>
> # 4. Generate the final class prediction
> class_logits = classification_head(final_representation)
> predicted_class = argmax(class_logits) # e.g., 0='Forest', 1='Corn', etc.
>
> # By running this for every pixel, we create a complete land cover map.
> ```

---

## Links to Practical Work
- [[Practical Work for satellite image analyist]]