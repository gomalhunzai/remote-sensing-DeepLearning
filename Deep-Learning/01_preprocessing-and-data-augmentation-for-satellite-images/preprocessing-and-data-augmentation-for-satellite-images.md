---
publishable: false
tags: []
date_updated: 2025-07-22
author: Generated by AI
---

# Preprocessing and Data Augmentation for Satellite Images
> **Learning Objective:** Master essential techniques for preparing satellite data, including atmospheric correction, georeferencing, tiling, normalization, and augmentation strategies specific to overhead imagery.

## Introduction
The axiom "garbage in, garbage out" is nowhere more critical than in the application of deep learning to satellite imagery. Raw data, as delivered from a satellite sensor, is a measurement of electromagnetic energy, not a direct representation of the Earth's surface. It is distorted by the sensor's viewing angle, the curvature of the Earth, and the scattering and absorption of light by the atmosphere. Before a neural network can learn to identify features like buildings, roads, or crop types, this raw data must be meticulously transformed into a consistent, analysis-ready format. This chapter addresses the critical bridge between raw sensor data and model-ready input tensors. We will cover the two principal phases of this process: first, the foundational preprocessing steps that correct for radiometric and geometric distortions, and second, the model-centric preparation involving structuring the data and artificially expanding the dataset to enhance model performance and robustness.

---

## Topic 1: From Raw Sensor Data to Analysis-Ready Pixels

The initial data from a satellite sensor, often referred to as Level-1 data, consists of Digital Numbers (DNs). These are unitless integer values that represent the intensity of radiance recorded by the sensor. DNs are not directly comparable between different sensors, or even between images taken by the same sensor at different times or under different atmospheric conditions. Our first objective is to convert these raw DNs into a physically meaningful, standardized measure: **surface reflectance**. This process involves two key steps: atmospheric correction and geometric correction.

### Atmospheric Correction
The atmosphere acts as a filter and a source of noise between the sun, the Earth's surface, and the satellite sensor. It scatters sunlight (creating haze) and absorbs energy at specific wavelengths (e.g., by water vapor). Atmospheric correction is the process of algorithmically removing these effects to estimate the true reflectance of the surface, as if there were no atmosphere.

*   **Why it's essential:** For a deep learning model to generalize, it must learn the intrinsic spectral signature of an object. A model trained on images of "healthy vegetation" from a clear, dry day will fail to recognize the same vegetation in an image from a hazy, humid day if the atmospheric effects are not removed. Correcting to surface reflectance ensures that a pixel value of 0.2 in the near-infrared band represents the same surface property, regardless of when or where the image was captured.
*   **Common Methods:** Models like 6S (Second Simulation of a Satellite Signal in the Solar Spectrum), FLAASH (Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes), or simpler methods like Dark Object Subtraction (DOS) are used. For many modern satellite data products (e.g., Landsat Collection 2 Level-2, Sentinel-2 Level-2A), this correction has already been applied by the data provider, yielding what is known as **Analysis-Ready Data (ARD)**.

### Georeferencing and Orthorectification
A raw satellite image is a distorted perspective view. Pixels do not align perfectly with a standard map grid due to sensor viewing angle, topographic variations on the ground, and the Earth's curvature.

*   **Georeferencing:** The process of assigning real-world geographic coordinates (e.g., latitude and longitude) to every pixel in the image.
*   **Orthorectification:** A more rigorous form of georeferencing that corrects for terrain displacement using a Digital Elevation Model (DEM). This ensures that a road running over a hill appears straight on the map, not distorted by the topography.

For deep learning, accurate orthorectification is paramount. It ensures that the spatial relationships between features are true to life and that an object's shape and size are consistent across the dataset. When fusing data from multiple sources or performing multi-temporal analysis, precise pixel alignment is non-negotiable.

> [!NOTE] Guided Application Exercise
> **Problem:** You have been provided with a raw Level-1T Landsat 9 scene of a coastal region. Your goal is to prepare it for a multi-temporal analysis to monitor coastal erosion over five years by comparing it with older, already-corrected images.
>
> **Resolution Process:**
> 1.  **Step 1: Radiometric Calibration.** The first step is to convert the raw Digital Numbers (DNs) from the Level-1T product into at-sensor spectral radiance. This involves applying the rescaling factors (gain and bias) provided in the image's metadata file (MTL.txt). This step converts the arbitrary integers into physical units (e.g., W/(m²⋅sr⋅µm)).
> 2.  **Step 2: Atmospheric Correction.** Using the calibrated radiance values and metadata about the scene's acquisition (e.g., solar zenith angle, acquisition time), apply an atmospheric correction model (like a Python implementation of DOS or a tool like Sen2Cor/LaSRC). This converts at-sensor radiance into surface reflectance, removing the effects of atmospheric haze and absorption. This is the most critical step for ensuring the pixel values are comparable to the older images.
> 3.  **Step 3: Geometric Verification.** The 'T' in Level-1T indicates the data is already terrain-corrected and orthorectified to a standard projection (e.g., UTM). You must verify that the projection and datum of your newly processed image match those of the historical images you are comparing against. If they differ, you would need to reproject one of the datasets to match the other.
>
> **Solution:**
> The final output is a multi-band GeoTIFF file where each pixel value represents surface reflectance, a unitless value typically between 0 and 1. The image is orthorectified and projected to a standard coordinate system. Now, a pixel at coordinates `(X, Y)` in this new image corresponds to the exact same ground location and has a physically comparable value as the pixels in the historical dataset, making a robust, automated change detection analysis possible.

---

## Topic 2: Structuring Data for Model Ingestion: Tiling and Normalization

Once you have an Analysis-Ready Data (ARD) product, it is still not ready to be fed into a deep learning model. A single satellite scene can be enormous (often several gigabytes) and have dimensions of many thousands of pixels on each side. Deep learning models, particularly Convolutional Neural Networks (CNNs), require fixed-size inputs and operate on data that is scaled to a specific numerical range.

### Tiling (or Patching)
Tiling is the process of systematically cropping a large satellite image into smaller, uniform-sized squares called tiles or patches (e.g., 256x256 or 512x512 pixels).

*   **Rationale:**
    1.  **Memory Constraints:** It is computationally infeasible to load an entire gigapixel scene into GPU memory. Tiling breaks the problem into manageable chunks.
    2.  **Fixed Model Input:** CNNs are designed with a fixed input tensor size. Tiling ensures every input fed to the model conforms to this required dimension.
    3.  **Increased Sample Size:** A single large image can generate thousands of smaller tiles, effectively increasing the number of training samples for the model.

A crucial consideration in tiling is **overlap**. When you create tiles without overlap, an object of interest (like a building) might be split exactly on the edge of two tiles. This prevents the model from ever seeing the full object in context. By creating tiles with a certain percentage of overlap (e.g., 25%), you ensure that every point in the original image is captured near the center of at least one tile.

### Normalization
Normalization is the process of scaling pixel values to a consistent range. This is a standard and vital step in virtually all deep learning workflows. It helps the model's optimization algorithm (like gradient descent) to converge faster and more reliably.

*   **Why it's essential:** Different spectral bands have vastly different value ranges. For example, visible bands might have reflectance values between 0.01 and 0.4, while a near-infrared band might range from 0.1 to 0.7. Without normalization, the network weights associated with the higher-range bands will dominate the learning process, leading to instability.
*   **Common Strategies:**
    1.  **Min-Max Scaling:** Scales all values to a fixed range, typically `[0, 1]`. The formula is: `x_scaled = (x - x_min) / (x_max - x_min)`. This is simple but sensitive to outliers.
    2.  **Standardization (Z-score Normalization):** Scales data to have a mean of 0 and a standard deviation of 1. The formula is: `x_scaled = (x - mean) / std_dev`. This is generally more robust and the preferred method for many deep learning applications.

**Critical Methodological Note:** The normalization statistics (mean, standard deviation, min, max) must be calculated *only from the training dataset*. These exact same statistics must then be applied to scale the validation and test datasets. Calculating statistics from the entire dataset and then splitting it introduces data leakage, where information from the test set "leaks" into the training process, leading to an over-optimistic evaluation of the model's performance.

> [!NOTE] Guided Application Exercise
> **Problem:** You have a 10,000 x 10,000 pixel, 4-band (Red, Green, Blue, Near-Infrared) analysis-ready aerial photograph of a city. You need to prepare this image for a semantic segmentation model that accepts input tensors of size 256x256x4.
>
> **Resolution Process:**
> 1.  **Step 1: Define Tiling Strategy.** Choose a tile size of 256x256 pixels. To avoid edge effects, decide on an overlap of 64 pixels (25% overlap). You will slide a 256x256 window across the large image, with a stride of 192 pixels (256 - 64) in both the x and y directions. For each window, you extract the 256x256x4 pixel data.
> 2.  **Step 2: Calculate Normalization Statistics.** Iterate through all the tiles designated for the *training set*. For each of the 4 bands, calculate the global mean and standard deviation across all pixels in all training tiles. This will result in four mean values (`mean_r`, `mean_g`, `mean_b`, `mean_nir`) and four standard deviation values (`std_r`, `std_g`, `std_b`, `std_nir`).
> 3.  **Step 3: Apply Normalization.** Create a function or data loader that, for every 256x256x4 tile (whether for training, validation, or testing), applies the pre-calculated statistics. For each pixel in the red band, it will compute `(pixel_r - mean_r) / std_r`, and so on for the other bands.
>
> **Solution:**
> The process results in a large collection of normalized data tiles. Each tile is a floating-point tensor of shape `(256, 256, 4)` where the pixel values are centered around 0 with a standard deviation of 1. This collection of tensors is now perfectly structured to be fed in batches to the deep learning model for training and evaluation.

---

## Topic 3: Geospatial Data Augmentation Strategies

Data augmentation is the technique of artificially increasing the size and diversity of a training dataset by applying a series of random transformations to the existing images. In remote sensing, where labeled data can be scarce and expensive to produce, augmentation is not just beneficial—it is often essential for building a model that generalizes well to new, unseen imagery.

While some standard augmentations from computer vision apply here, satellite imagery has unique properties that allow for—and prohibit—certain types of transformations. The key is to apply transformations that are plausible in the real world from an overhead perspective.

### Standard and Geospatially-Aware Augmentations
1.  **Geometric Augmentations:**
    *   **Flips:** Horizontal and vertical flips are almost always safe and effective.
    *   **Rotations:** Unlike many natural images (e.g., a photo of a person), objects in satellite images do not have a canonical "up" direction. A car, a building, or a field can be oriented at any angle. Therefore, in addition to simple 90-degree rotations, **arbitrary rotations** (e.g., a random angle between 0 and 360 degrees) are extremely powerful for satellite imagery. This teaches the model rotational invariance.

2.  **Color and Radiometric Augmentations:**
    *   **Brightness & Contrast:** Randomly adjusting the brightness and contrast of an image simulates variations in illumination due to time of day or sun angle.
    *   **Hue & Saturation:** Minor adjustments can simulate slight atmospheric differences or seasonal changes in vegetation.
    *   **Adding Noise:** Adding a small amount of Gaussian noise to the image can make the model more robust to sensor noise.

### Augmentations to Avoid
*   **Heavy Shearing:** While a slight shear might be acceptable, significant shearing can distort the geometric properties of features in a way that is physically unrealistic from an overhead perspective.
*   **Unrealistic Color Space Shifts:** Drastically changing the color balance in a way that violates the typical spectral signatures of materials can confuse the model.
*   **Perspective Transforms:** These are generally not applicable as orthorectification has already removed perspective distortion.

When applying augmentations to a segmentation task, it is critical that the **exact same geometric transformation** is applied to both the input image tile and its corresponding label mask.

> [!NOTE] Guided Application Exercise
> **Problem:** You are training a CNN to detect and classify small-scale agricultural fields from 256x256 RGB tiles. Your initial training set is only 1,000 tiles, and your model is showing signs of overfitting. How would you design an augmentation pipeline?
>
> **Resolution Process:**
> 1.  **Step 1: Identify Core Augmentations.** Start with the safest and most effective transformations. Because field orientation is arbitrary, you should include horizontal flips, vertical flips, and arbitrary rotations.
> 2.  **Step 2: Add Color/Radiometric Augmentations.** To account for different lighting conditions and seasonality, add random adjustments to brightness and contrast. A small amount of color jitter (hue/saturation) can also help.
> 3.  **Step 3: Implement the Pipeline.** Use a library like `Albumentations`, which is highly optimized for this. The pipeline would be defined to apply a sequence of these transformations, each with a certain probability. For example, a flip might be applied to 50% of the images, while a random rotation is applied to all of them.
>
> **Solution:**
> An effective augmentation pipeline, expressed in pseudo-code using the `Albumentations` library syntax, would look like this:
> ```python
> import albumentations as A
>
> augmentation_pipeline = A.Compose([
>     # Apply flips with a 50% probability
>     A.HorizontalFlip(p=0.5),
>     A.VerticalFlip(p=0.5),
>     
>     # Apply a random rotation between -180 and 180 degrees
>     A.Rotate(limit=180, p=1.0),
>     
>     # Randomly adjust brightness and contrast
>     A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),
>     
>     # Add a bit of blur or noise occasionally
>     A.GaussianBlur(p=0.1),
>     A.GaussNoise(p=0.1)
> ], p=1.0)
> ```
> By applying this pipeline to your 1,000 tiles during training, the model will see a virtually infinite number of unique variations, forcing it to learn the true underlying features of agricultural fields rather than memorizing the specific orientations and lighting in the original small dataset.

---

## Links to Practical Work
- [[Practical Work for satellite image analyist]]