---
publishable: false
tags: []
date_updated: 2025-07-22
author: Generated by AI
---

# Working with Multispectral and SAR Data
> **Learning Objective:** Learn techniques to fuse and analyze data beyond the visible spectrum, incorporating multispectral, hyperspectral, and Synthetic Aperture Radar (SAR) imagery for enhanced insights.

## Introduction
While the human eye perceives the world in three bands of light—red, green, and blue (RGB)—satellite sensors offer a view that extends far beyond this limitation. This chapter moves past the familiar territory of RGB imagery to explore the rich information contained in the non-visible portions of the electromagnetic spectrum. We will delve into multispectral and hyperspectral data, which capture nuanced spectral "fingerprints" of materials on the Earth's surface, allowing us to differentiate between crop types, assess forest health, or identify mineral deposits with incredible precision.

Furthermore, we will introduce an entirely different sensing modality: Synthetic Aperture Radar (SAR). Unlike passive optical sensors that rely on sunlight, SAR is an active system that illuminates the Earth with its own microwave energy. This allows it to "see" through clouds and darkness, providing invaluable information about surface structure, roughness, and moisture content. The core mission of this chapter is to equip you with the foundational knowledge and techniques to preprocess, interpret, and, most importantly, fuse these disparate data types. By combining the "what" from optical sensors with the "how it's structured" from SAR, we can build more robust and insightful deep learning models for complex environmental analysis.

---

## Topic 1: The Spectral Dimension: Multispectral and Hyperspectral Data
Standard color images are a form of multispectral data, but they are limited to three broad bands in the visible spectrum. True multispectral (MS) and hyperspectral (HS) sensors extend this capability significantly.

*   **Multispectral (MS) Imagery:** Captures data in a small number of spectral bands (typically 3 to 15) that are strategically placed across the electromagnetic spectrum. For example, the Sentinel-2 satellite captures 13 bands, including several in the visible, near-infrared (NIR), and short-wave infrared (SWIR) regions. Each band is designed to be sensitive to a specific phenomenon, such as chlorophyll absorption, water content, or atmospheric conditions.

*   **Hyperspectral (HS) Imagery:** Takes this concept to the extreme, capturing data in hundreds of narrow, contiguous spectral bands. The result is not just a series of discrete images, but a continuous spectrum for every pixel in the scene. This "spectral signature" is like a detailed fingerprint for different materials, enabling extremely fine-grained classification.

**From Raw Data to Actionable Insights: Preprocessing and Spectral Indices**

Before this rich spectral information can be fed into a deep learning model, it must be properly preprocessed. Raw satellite data is recorded in Digital Numbers (DN), which are unitless values that need to be converted into physically meaningful quantities.

1.  **Radiometric Calibration:** This process converts DNs to at-sensor radiance, a measure of the energy received by the sensor.
2.  **Atmospheric Correction:** This crucial step removes the distorting effects of the atmosphere (e.g., scattering and absorption by water vapor and aerosols). The goal is to convert at-sensor radiance to surface reflectance—the proportion of incoming solar radiation that is reflected by the Earth's surface. This standardization is essential for comparing images taken at different times or by different sensors.

Once we have surface reflectance data, we can use it to engineer powerful features for our models. Instead of feeding all the raw bands into a network, we can calculate **spectral indices**. These are simple mathematical combinations of different bands designed to highlight a specific feature. The most famous is the **Normalized Difference Vegetation Index (NDVI)**, which leverages the fact that healthy vegetation strongly reflects near-infrared (NIR) light and strongly absorbs red light.

The formula is:
`NDVI = (NIR - Red) / (NIR + Red)`

The resulting values range from -1 to +1, where high positive values (e.g., > 0.6) indicate dense, healthy vegetation, values near zero represent bare soil or rock, and negative values typically correspond to water.

> [!NOTE] Guided Application Exercise
> **Problem:** You are given surface reflectance values for a single pixel from a Sentinel-2 satellite image over a farm. The Red band (Band 4) has a reflectance of 0.08, and the Near-Infrared band (Band 8) has a reflectance of 0.65. Calculate the NDVI for this pixel and interpret the result.
>
> **Resolution Process:**
> 1.  **Identify Inputs:** We need the reflectance values for the NIR and Red bands.
>    *   `NIR = 0.65`
>    *   `Red = 0.08`
> 2.  **Apply the NDVI Formula:** Substitute the values into the formula: `NDVI = (NIR - Red) / (NIR + Red)`.
>    *   `NDVI = (0.65 - 0.08) / (0.65 + 0.08)`
> 3.  **Calculate the Result:**
>    *   `NDVI = 0.57 / 0.73`
>    *   `NDVI ≈ 0.78`
>
> **Solution:**
> The calculated NDVI for the pixel is approximately **0.78**.
>
> **Interpretation:** An NDVI value of 0.78 is a high positive value. This strongly suggests that the pixel represents an area of dense and healthy vegetation, likely a thriving crop within the farm. This single, engineered feature is a much more direct indicator of vegetation health than the raw band values alone.

---

## Topic 2: The Structural Dimension: Synthetic Aperture Radar (SAR)
Synthetic Aperture Radar (SAR) operates on fundamentally different principles from optical sensors. As an **active sensor**, it transmits its own microwave pulses and records the energy that is scattered back, known as **backscatter**. This self-illumination gives SAR two major advantages: it can acquire imagery day or night, and its long-wavelength microwaves can penetrate clouds, haze, and rain, guaranteeing data acquisition regardless of weather conditions.

What SAR "sees" is not color, but rather the physical properties of the surface:

*   **Surface Roughness:** Smooth surfaces, like calm water or paved runways, act like mirrors and reflect the radar pulse away from the sensor, resulting in very low backscatter (dark pixels). Rough surfaces, like a forest canopy or choppy water, scatter the energy in many directions, including back to the sensor, resulting in high backscatter (bright pixels).
*   **Geometric Structure:** The shape and orientation of objects have a huge impact. Urban areas are famously bright in SAR images due to a phenomenon called **double bounce**, where the radar pulse bounces from the street to a building wall and then back to the sensor.
*   **Dielectric Constant:** This property is closely related to moisture content. Wet soil has a higher dielectric constant than dry soil, causing it to reflect more energy and appear brighter in a SAR image. This makes SAR exceptionally useful for flood mapping and soil moisture estimation.

**SAR-Specific Preprocessing**

SAR data comes with its own unique set of challenges that must be addressed before analysis.

1.  **Speckle Reduction:** SAR imagery is inherently affected by "speckle," a grainy, salt-and-pepper noise resulting from the coherent interference of the radar waves. This is not sensor noise but a real property of the imaging process. It must be reduced using specialized filters (e.g., Lee filter, Gamma MAP filter) to improve interpretability and the performance of deep learning algorithms.
2.  **Radiometric Calibration:** Similar to optical data, raw SAR data must be calibrated to convert the digital numbers into a standardized measure of backscatter, typically the backscatter coefficient Sigma Nought ($\sigma\degree$). This ensures that brightness values are related to the physical properties of the target, not the sensor's viewing geometry.
3.  **Terrain Correction:** Because SAR is a side-looking system, it suffers from significant geometric distortions caused by terrain, such as **layover** (where mountain tops appear to lean towards the sensor) and **shadow**. Terrain correction uses a Digital Elevation Model (DEM) to reposition pixels to their correct geographic locations, producing a map-accurate image.

> [!NOTE] Guided Application Exercise
> **Problem:** An analyst is examining a pre-processed SAR image (calibrated and terrain-corrected) showing a landscape with a river, a forest, and a small town. How would they differentiate these three features based on their appearance in the SAR image?
>
> **Resolution Process:**
> 1.  **Analyze the River:** A river is a body of water. Assuming it is calm, its surface will be very smooth relative to the radar wavelength. Therefore, it will act as a specular reflector, bouncing most of the radar energy away from the sensor. It should appear as a dark, linear feature in the image.
> 2.  **Analyze the Forest:** A forest canopy is a very rough surface with countless leaves, twigs, and branches at various orientations. This is known as a volume scatterer. It will scatter the radar energy in all directions, with a significant portion returning to the sensor. The forest should appear as a bright, textured area.
> 3.  **Analyze the Town:** The town is composed of buildings and streets. This creates many right-angle structures. The radar signal will hit the flat street, bounce up to the vertical wall of a building, and then be reflected directly back to the sensor (double bounce). This is a very efficient reflection mechanism, causing urban areas to appear as a collection of extremely bright, often grid-like, pixels.
>
> **Solution:**
> The analyst would interpret the image as follows:
> *   **Dark, winding shapes:** These are areas of low backscatter, corresponding to the **river**.
> *   **Expansive areas of medium-to-high brightness with a granular texture:** These are areas of high surface roughness and volume scattering, corresponding to the **forest**.
> *   **Clusters of exceptionally bright, distinct pixels:** These are areas of strong double-bounce backscatter, corresponding to the buildings in the **town**.

---

## Topic 3: Data Fusion: Combining Optical and SAR for Enhanced Analysis
While multispectral and SAR data are powerful on their own, their true potential is unlocked when they are used together. This process, known as **data fusion**, aims to create a single, richer dataset that leverages the complementary nature of the two sensor types. Optical data tells us about the *spectral properties* of a target (e.g., this is vegetation), while SAR data tells us about its *structural and dielectric properties* (e.g., this vegetation is tall and dense, or the ground beneath it is wet).

For deep learning, the most common and effective method is **pixel-level fusion**, often implemented as a simple **data stack**. The concept is to create a single multi-channel "super-image" where each pixel contains information from both sensors.

The workflow for creating a fused data stack is as follows:

1.  **Preprocessing:** Each dataset (optical and SAR) must first be individually preprocessed as described in the previous topics. This means the optical data is atmospherically corrected to surface reflectance, and the SAR data is speckle-filtered, calibrated, and terrain-corrected.
2.  **Co-registration:** This is the most critical step. The optical and SAR images must be precisely aligned so that each pixel in one image corresponds to the exact same geographic location on the ground in the other image. A slight misalignment can cause a model to associate the SAR backscatter of a building with the spectral signature of an adjacent tree, leading to poor performance.
3.  **Resampling:** The two datasets will likely have different native spatial resolutions (pixel sizes). For example, Sentinel-2 has bands at 10m, 20m, and 60m, while Sentinel-1 data is often processed to a 10m pixel size. All data must be resampled to a common grid, typically the finest resolution available (e.g., 10m).
4.  **Stacking:** Once aligned and on a common grid, the bands from both sensors are concatenated into a single data cube. If you have 10 bands from Sentinel-2 and 2 polarization channels (e.g., VV and VH) from Sentinel-1, your final fused product will be a 12-channel image.

This 12-channel tensor can then be fed directly into a deep learning architecture, such as a Convolutional Neural Network (CNN). The network will learn to extract features not just from the spectral or structural domains individually, but from the complex interactions between them, enabling far more sophisticated and accurate classification or segmentation than would be possible with a single sensor.

> [!NOTE] Guided Application Exercise
> **Problem:** You are tasked with designing the input data for a CNN to map crop types. You have access to Sentinel-2 (multispectral) and Sentinel-1 (SAR) data. Describe the structure of the final input tensor that will be fed into the model for a single `256x256` pixel image patch.
>
> **Resolution Process:**
> 1.  **Select Bands/Channels:**
>     *   **Sentinel-2:** We will use the 10 bands available at 10m and 20m resolution (we will ignore the 60m atmospheric bands). These include Blue, Green, Red, 3 Red-Edge bands, NIR, and 2 SWIR bands.
>     *   **Sentinel-1:** We will use the two standard polarization channels: VV (vertical transmit, vertical receive) and VH (vertical transmit, horizontal receive).
> 2.  **Define Preprocessing & Fusion Workflow:**
>     *   Apply atmospheric correction to Sentinel-2 data to get surface reflectance.
>     *   Apply speckle filtering, calibration, and terrain correction to Sentinel-1 data.
>     *   Resample all 10 selected Sentinel-2 bands to a common 10m resolution.
>     *   Co-register the preprocessed Sentinel-1 data to the Sentinel-2 data grid.
> 3.  **Stack the Channels:** Combine the channels into a single file. The order can be arbitrary, but must be consistent. A common approach is to place the optical bands first, followed by the radar channels.
>     *   `Channel 1-10`: Sentinel-2 bands (B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12)
>     *   `Channel 11`: Sentinel-1 VV polarization
>     *   `Channel 12`: Sentinel-1 VH polarization
>
> **Solution:**
> The final input for the CNN will be a single tensor representing the `256x256` pixel image patch. The shape of this tensor will be **`(256, 256, 12)`**.
>
> *   `256`: The height of the image patch.
> *   `256`: The width of the image patch.
> *   `12`: The number of channels, comprising 10 from Sentinel-2 and 2 from Sentinel-1.
>
> This 12-channel tensor provides the model with a rich, multi-modal view of the target area at each pixel location, combining spectral and structural information for superior crop type classification.

---

## Links to Practical Work
- [[Practical Work for satellite image analyist]]